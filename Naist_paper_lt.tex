
\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}
\usepackage{multirow}
\usepackage{float}
\usepackage{amsmath}
\usepackage[section]{placeins}
\usepackage{amsthm}
\usepackage{eucal}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{pgfplotstable}
\usepackage{pgfplots}
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
\title{Performance Analysis of Parallel Collaborative Filtering on Spark and Hadoop cluster}

\author{\IEEEauthorblockN{Kundjanasith Thonglek}
\IEEEauthorblockA{High Performance Computing and Network Center\\
Deparment of Computer Engineering, Kasetsart University\\
Bangkok, Thailand\\
kundjanasith.t@ku.th}
\and
\IEEEauthorblockN{Kohei Ichikawa}
\IEEEauthorblockA{Software Design and Analysis Laboratory\\
Graduate School of Information Science, NAIST\\
Nara, Japan\\
ichikawa@is.naist.jp}}
%\and
%\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%\IEEEauthorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}
\maketitle


\begin{abstract}
Spark is a cluster computing technology for large-scale data processing that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Following the SparkContext can connect to several types of cluster manager which allocate resource across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for the application. Next, it sends application code to the executors. SparkContext represents the connection to a Spark cluster and can be used to create resilient distributed datasets (RDDs). it also accumulators and broadcast variables on the cluster. This research approaches the problem by comparing Spark cluster and Hadoop cluster performance are used the parallel collaborative filtering program?s execution time as criteria to find when the application should execute on Spark or Hadoop. This research is primarily intended for cluster computing benchmark.
\end{abstract}

\begin{IEEEkeywords}
Large scale data processing; Resilient Distributed Datasets; Collaborative Filtering; Spark;
\end{IEEEkeywords}


\section{Introduction}
Big data and advanced analytics technology is not just because the size of data is big but also because the potential of impact is big such as recommendation system, clustering system and the others. They were not possible previously because they were too costly to implement or they were not capable of handling the large volumes of data involved in a timely manner. But in the present technology can make it possible due to the new data management systems that handle a wide variety of data from sensor data to web and social media data improved analytical capabilities including event, predictive and text analytics, faster hardware ranging from faster multi-core processors and large memory space to solid-state drives and tried data storage for handling data. Supporting big data involves combining these technologies to enable new solutions that can bring significant benefits to the business.\\
\indent Recommendation system is the key to success for discovering and retrieval of content in this era of big data. It have become increasingly popular in recent years, and are utilized in a variety of areas including movies, music, news, books, and product in general. These are also recommendation for experts. Collaborative Filtering (CF) technique is used to create recommendation system, is a technique of making automatic predictions about interest of a user by collecting preference or taste information from many users. The advantages of this technique are not rely on machine analyzable content, it is capable of accurately recommending complex items such as movies movies without requiring an understanding of the item similarity in recommendation system. The technique is based on the assumption that people who agreed in the past will agree in the future, and they will like similar kinds of items as they liked in the past. The algorithm is widely applied to this technique is Alternating Least Square (ALS) algorithm that aim at learning the two types of items as they liked in the past.\\
\indent Large-scale data processing is the process of applying data analysis technique to a large amount of data. Typically, large-scale data analysis is performed through two popular techniques: parallel database management systems or map reduce powers systems. The parallel database management system requires that the data be in a database management system supported schema, whereas map reduce system supports data in any form. Moreover, the dat extracted or analyzed in large-scale data analysis can be displayed in various different forms such as tables, graphs, figures and statistical analysis that depending on the analysis system. There are two popular open source cluster computing for large-scale data processing based on the map reduce model: Spark framework which is the framework process data by in-memory processing, has Spark ML library to be scalable machine learning library and Hadoop framework which is the framework process data by on-disk processing, has Mahout library to be scalable machine learning library.

%\subsection{Subsection Heading Here}
%\blindtext

\section{Literature review}
\subsection{Large-scale Parallel Collaborative Filtering for Netflix}
Spark ML library and Mahout library are scalable machine learning library use this paper to be the reference for parallel collaborative filtering technique using alternating least square algorithm to implement in the library of both. However, this paper execute the parallel collaborative filtering technique using Matlab library on Linux cluster only. \\
\indent This research will further advance paper by executing parallel collaborative filtering technique using more efficient framework than using Matlab library on Linux cluster with Spark ML library on Spark cluster and Mahout library on Hadoop cluster. 

\subsection{Comparing Apache Spark and MapReduce with Performance Analysis using K-Means}
K-means clustering algorithm is the algorithm that simpler than Alternating Least Square algorithm is used to apply with parallel collaborative filtering technique. Spark and Hadoop cluster with 1 node and 2 nodes is too small to find the impact of the number of node on the executing performance. \\
\indent This research will further advance paper by using the alternating least square algorithm of parallel collaborative technique instead of K-means algorithm because of the complexity. The environment of experimental cluster is extended to 5 worker nodes that can find the impact or relation to the performance with increasing the number of worker nodes. 

\section{Design and Implementation}
There are 2 large-scale data processing framework which we use in the experiment are Spark framework and Hadoop framework. Spark using in-memory processing to process data but Hadoop using on-disk processing. \\
\indent  Alternating Least Square (ALS) algorithm is CPU intensive algorithm more than memory intensive so we design experiment by vary the number of CPU cores per each worker node with fix the size of memory per each worker node. We interest the number of worker nodes effect to the performance or not then we design the experiment by changing the number of worker nodes in the cluster with the same total number of CPU cores and memory size in the cluster. \\
\indent We find the dataset that suitable to create recommendation system by parallel collaborative filtering technique is dataset from The Netflix Prize that is a large-scale data mining competition held by Netflix for the best recommendation system algorithm for predicting user ratings on movies, based on a training set of more than 100 million ratings given by over 480,000 users to nearly 18,000 movies. Each training data point consist of a quadruple ( user, movie, date, rating ) where the rating is an integer from 1 to 5, the size of dataset is 2.8 gb. Afterward, we separate the dataset using sampling to 20 datasets by starting from 5 million records then add up 5 million util 100 million records. \\
\indent Experiment is divided into 2 sections to find the suitable configuration of Spark cluster and Hadoop cluster in the limited resource to executer parallel collaborative filtering technique for create the recommendation system: First section is the number of CPU cores per each worker node effect analysis to observe parallel computing by changing the number of CPU cores per each worker node to 2, 4, 8, 16 with fix the size of memory is 8 gb per each worker node and Second section is the number of worker node effect analysis to observer parallel I/O by changing the number of worker node with the total number of CPU cores is 20 and size of memory is  40 gb. \\
\indent Implementation is divided into three main parts in this experiment. The first part is installation part, the second part is training data part, the third part is performance analysis part. Installation part is a  part for installation and configuration the Spark and Hadoop on cluster. Training data part is a part for training data  for train data by Spark ML library on the Spark cluster and Mahout library on Hadoop cluster.  Performance analysis part is a part for analyst the execution time of each configuration. 

\subsection{Installation Part}
Installation Part has 2 sections: First section is the installation part for observe parallel computing effect and second section is the installation part for observe parallel I/O effect. \\
\indent The configuration in cluster of parallel computing observation:
\begin{itemize}
\item Case 1 : 5 worker nodes by each node has CPU 2 cores and memory 8 gb
\item Case 2 : 5 worker nodes by each node has CPU 4 cores and memory 8 gb
\item Case 3 : 5 worker nodes by each node has CPU 8 cores and memory 8 gb
\item Case 4 : 5 worker nodes by each node has CPU 16 cores and memory 8 gb
\end{itemize}
\indent The configuration in cluster of parallel I/O observation:
\begin{itemize}
\item Case 1 : 5 worker nodes by each node has CPU 4 cores and memory 8 gb
\item Case 2 : 4 worker nodes by each node has CPU 5 cores and memory 10 gb
\item Case 3 : 2 worker nodes by each node has CPU 10 cores and memory 20 gb
\item Case 4 : 1 worker node which has CPU 20 cores and memory 40 gb
\end{itemize}

\subsection{Training data Part}
Training data Part has 2 sections: Using Spark ML library on Spark cluster and Mahout library on Hadoop cluster which have the same alternating least square algorithm implementation on both libraries. \\
\indent The Parallel Alternating-Least-Squares with Weighted-\begin{math}\lambda\end{math}-Regularization (ALS-WR) algorithm  is parallelized by parallelizing the updates of the user feature matrix and of the movie feature matrix.


\subsection{Performance analysis Part}
Performance analysis Part has 3 sections: Recording time information using time command,  Calculate the average value of time in second unit due to we give a trial three times per each condition and evaluate the result from the experiment by R programming language.\\
\indent For the number of CPU cores per each worker node effect analysis to observer parallel computing. There are 2 dependent variables on time variable: the number of CPU cores per each worker node and the size of dataset.\\
\indent Let \textbf{\textit{t}} be the execution time (seconds)\\
\indent Let \textbf{\textit{c}} be the number of CPU core per worker node (cores)\\
\indent Let \textbf{\textit{d}} be the size of data set (MB)\\
\indent The Multiple linear regression (MLR) model that describes a dependent variable  \textbf{\textit{t}} by independent variables \textbf{\textit{c}} and  \textbf{\textit{d}} is expressed by the equation as follow where the number \begin{math}\alpha\end{math} and \begin{math}\beta\end{math} are the parameters (1).

\indent For the number of worker nodes effect analysis to observer parallel I/O. There are 2 dependent variables on time variable: the number of worker nodes and the size of dataset.\\
\indent Let \textbf{\textit{t}} be the execution time (seconds)\\
\indent Let \textbf{\textit{n}} be the number of worker node (nodes)\\
\indent Let \textbf{\textit{d}} be the size of data set (MB)\\
\indent The Multiple linear regression (MLR) model that describes a dependent variable  \textbf{\textit{t}} by independent variables \textbf{\textit{n}} and  \textbf{\textit{d}} is expressed by the equation as follow where the number \begin{math}\alpha\end{math} and \begin{math}\beta\end{math} are the parameters (2).

\begin{equation}
	t = \alpha_{1} + \beta_{1}c + \beta_{2}d
\end{equation}
\begin{equation}
	t = \alpha_{2} + \beta_{3}n + \beta_{4}d 
\end{equation}

\section{Experiment result}
After we give a trial for execute the different size of data sets with the same algorithm but different type of cluster configuration. The experimental result is divided to 2 sections.\\
\indent First section is the experimental result of executing parallel collaborative filtering using alternating least square algorithm with Spark ML library on Spark cluster and Mahout library on Hadoop cluster by changing the number of CPU cores per worker node in the cluster which has 1 master node and 5 worker nodes for observe the impact from parallel computing on the performance. 

\begin{figure}[h]
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	xtick={139.90, 279.80, 419.71, 559.61, 699.51, 839.42},
	grid=both,
	legend pos=north west]
%\addplot table [y=$Hadoop$, x=Size]{latex_data/HS_C1.dat};
\addplot[color=blue,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 463.88)
         (279.80, 920.81)
         (419.71, 1437.65)
         (559.61, 1900.33)
         (699.51, 2337.72)
         (839.42, 2753.91)
    }; 
\node [below] at (axis cs:  139.90, 463.88) {$463.88$};
\node [below] at (axis cs:  279.80, 920.81) {$920.81$};
\node [left] at (axis cs: 419.71, 1437.65) {$1437.65$};
\node [left] at (axis cs: 559.61, 1900.33) {$1900.33$};
\node [left] at (axis cs: 699.51, 2337.72) {$2337.72$};
\node [left] at (axis cs: 839.42, 2753.91) {$2753.91$};
\addlegendentry{$Hadoop$  cluster}
%\addplot table [y=$Spark$, x=Size]{latex_data/HS_C1.dat};
\addplot[color=red,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 1018.69)
         (279.80, 1061.27)
         (419.71, 1099.85)
         (559.61, 1154.24)
         (699.51, 1208.77)
         (839.42, 1338.82)
    }; 
\node [below] at (axis cs:  139.90, 1018.69) {$1018.69$};
\node [above] at (axis cs:  279.80, 1061.27) {$1061.27$};
\node [below] at (axis cs: 419.71, 1099.85) {$1099.85$};
\node [above] at (axis cs: 559.61, 1154.24) {$1154.24$};
\node [below] at (axis cs: 699.51, 1208.77) {$1208.77$};
\node [above] at (axis cs: 839.42,1338.82) {$1338.82$};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 5 worker nodes by each node has CPU 2 cores and memory 8 GB}
\end{figure}
From Fig 1, we can analyze that the intersection point between Hadoop cluster line and Spark cluster line at the size of dataset is  319.51 MB and the execution time is 1074.05 seconds. As the result, we can execute parallel collaborative filtering on Hadoop cluster faster than Spark cluster when the size of input data is less than 319.51 MB by 5 worker nodes by each node has CPU 2 cores and memory 8 gb.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ),
	xtick={139.90, 279.80, 419.71, 559.61, 699.51, 839.42},
	grid=both, 
	legend pos=north west]
%\addplot table [y=$Hadoop$, x=Size]{latex_data/HS_C2.dat};
\addplot[color=blue,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 254.23)
         (279.80, 593.71)
         (419.71, 906.54)
         (559.61, 1253.17)
         (699.51, 1468.55)
         (839.42, 1831.36)
    }; 
\node [below] at (axis cs:  139.90, 254.23) {$254.23$};
\node [right] at (axis cs:  279.80, 593.71) {$593.71$};
\node [right] at (axis cs: 419.71, 906.54) {$906.54$};
\node [left] at (axis cs: 559.61, 1253.17) {$1253.17$};
\node [left] at (axis cs: 699.51, 1468.55) {$1468.55$};
\node [left] at (axis cs: 839.42, 1831.36) {$1831.36$};
\addlegendentry{$Hadoop$  cluster}
%\addplot table [y=$Spark$, x=Size]{latex_data/HS_C2.dat};
\addplot[color=red,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 859.70)
         (279.80, 907.73)
         (419.71, 959.61)
         (559.61, 1154.24)
         (699.51, 1208.77)
         (839.42, 1338.82)
    }; 
\node [below] at (axis cs:  139.90, 859.70) {$859.70$};
\node [above] at (axis cs:  279.80, 907.73) {$907.73$};
\node [above] at (axis cs: 419.71, 959.61) {$959.61$};
\node [right] at (axis cs: 559.61, 1042.10) {$1042.10$};
\node [right] at (axis cs: 699.51, 1112.97) {$1112.97$};
\node [above] at (axis cs: 839.42,1196.68) {$1196.68$};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 5 worker nodes by each node has CPU 4 cores and memory 8 GB}
\end{figure}
From Fig 2, we can analyze that the intersection point between Hadoop cluster line and Spark cluster line at the size of dataset is 462.38 MB and the execution time is 990.78 seconds. As the result, we can execute parallel collaborative filtering on Hadoop cluster faster than Spark cluster when the size of input data is less than 462.38 MB by 5 worker nodes by each node has CPU 4 cores and memory 8 gb.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	xtick={139.90, 279.80, 419.71, 559.61, 699.51, 839.42},
	grid=both, 
	legend pos=north west]
%\addplot table [y=$Hadoop$, x=Size]{latex_data/HS_C3.dat};
\addplot[color=blue,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 174.88)
         (279.80, 351.64)
         (419.71, 550.53)
         (559.61, 727.93)
         (699.51, 921.13)
         (839.42, 1092.84)
    }; 
\node [below] at (axis cs:  139.90, 174.88) {$174.88$};
\node [right] at (axis cs:  279.80, 351.64) {$351.64$};
\node [right] at (axis cs: 419.71, 550.53) {$550.53$};
\node [right] at (axis cs: 559.61, 727.93) {$727.93$};
\node [left] at (axis cs: 699.51, 921.13) {$921.13$};
\node [above] at (axis cs: 839.42, 1092.84) {$1092.84$};
\addlegendentry{$Hadoop$  cluster}
%\addplot table [y=$Spark$, x=Size]{latex_data/HS_C3.dat};
\addplot[color=red,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 613.34)
         (279.80, 663.38)
         (419.71, 741.04)
         (559.61, 833.40)
         (699.51, 903.04)
         (839.42, 993.54)
    }; 
\node [below] at (axis cs:  139.90, 613.34) {$613.34$};
\node [above] at (axis cs:  279.80, 663.38) {$663.38$};
\node [below] at (axis cs: 419.71, 741.04) {$741.04$};
\node [left] at (axis cs: 559.61, 833.40) {$833.40$};
\node [right] at (axis cs: 699.51, 903.04) {$903.04$};
\node [above] at (axis cs: 839.42, 993.54) {$993.54$};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 5 worker nodes by each node has CPU 8 cores and memory 8 GB}
\end{figure}
From Fig 3, we can analyze that the intersection point between Hadoop cluster line and Spark cluster line at the size of dataset is 691.07 MB and the execution time is 902.86 seconds. As the result, we can execute parallel collaborative filtering on Hadoop cluster faster than Spark cluster when the size of input data is less than 691.07 MB by 5 worker nodes by each node has CPU 8 cores and memory 8 gb.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	xtick={139.90, 279.80, 419.71, 559.61, 699.51, 839.42},
	grid=both, 
	legend pos=north west]
%\addplot table [y=$Hadoop$, x=Size]{latex_data/HS_C4.dat};
\addplot[color=blue,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 160.47)
         (279.80, 323.76)
         (419.71, 438.17)
         (559.61, 549.12)
         (699.51, 678.46)
         (839.42, 818.09)
    }; 
\node [below] at (axis cs:  139.90, 160.47) {$160.47$};
\node [right] at (axis cs:  279.80, 323.76) {$323.76$};
\node [right] at (axis cs: 419.71, 438.17) {$438.17$};
\node [right] at (axis cs: 559.61, 549.12) {$549.12$};
\node [right] at (axis cs: 699.51, 678.46) {$678.46$};
\node [above] at (axis cs: 839.42, 818.09) {$818.09$};
\addlegendentry{$Hadoop$  cluster}
%\addplot table [y=$Spark$, x=Size]{latex_data/HS_C4.dat};
\addplot[color=red,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 409.36)
         (279.80, 475.68)
         (419.71, 543.79)
         (559.61, 612.68)
         (699.51, 708.02)
         (839.42, 798.31)
    }; 
\node [below] at (axis cs:  139.90, 409.36) {$409.36$};
\node [left] at (axis cs:  279.80, 475.6) {$475.68$};
\node [left] at (axis cs: 419.71, 543.79) {$543.79$};
\node [left] at (axis cs: 559.61, 612.68) {$612.68$};
\node [left] at (axis cs: 699.51, 708.02) {$708.02$};
\node [below] at (axis cs: 839.42, 798.31) {$798.31$};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 5 worker nodes by each node has CPU 16 cores and memory 8 GB}
\end{figure}
From Fig 4, we can analyze that the intersection point between Hadoop cluster line and Spark cluster line at the size of dataset is 759.62 MB and the execution time is 740.75 seconds. As the result, we can execute parallel collaborative filtering on Hadoop cluster faster than Spark cluster when the size of input data is less than 759.62 MB by 5 worker nodes by each node has CPU 16 cores and memory 8 gb.

\begin{table}[h]
\centering
\caption{The gradient of cluster line by each configuration that observe parallel computing }
\label{my-label}
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{The configuration in cluster} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Hadoop\\cluster\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Spark\\cluster\end{tabular}} \\
 &  &  \\ \hline
\begin{tabular}[c]{@{}c@{}}5 worker nodes by each node \\ has CPU 2 cores and memory 8 gb\end{tabular} & 3.27 & 0.46 \\ \hline
\begin{tabular}[c]{@{}c@{}}5 worker nodes by each node\\ has CPU 4 cores and memory 8 gb\end{tabular} & 2.25 & 0.48 \\ \hline
\begin{tabular}[c]{@{}c@{}}5 worker nodes by each node \\ has CPU 8 cores and memory 8 gb\end{tabular} & 1.31 & 0.54 \\ \hline
\begin{tabular}[c]{@{}c@{}}5 worker nodes by each node\\ has CPU 16 cores and memory 8 gb\end{tabular} & 0.94 & 0.56 \\ \hline
\end{tabular}
\end{table}

After execute the multiple linear regression algorithm as the equation (1), we discover that the coefficient of size of dataset is 0.5056 on Spark cluster and 1.938 on Hadoop cluster, the coefficient of the number of CPU cores per each worker node is -38.5424 on Spark cluster and -70.894 on Hadoop cluster, the interception is 927.1458 on Spark cluster and 537.263 on Hadoop cluster.

\indent Second section is the experimental result of executing parallel collaborative filtering using alternating least square algorithm with Spark ML library on Spark cluster and Mahout on Hadoop cluster by changing the number of worker in the cluster nodes which has the total number of CPU cores is 20 cores and the total number of memory is 40 GB for observe the impact from parallel I/O on the performance.
\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	xtick={139.90, 279.80, 419.71, 559.61, 699.51, 839.42},
	grid=both, 
	legend pos=north west]
\addplot[color=blue,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 254.23)
         (279.80, 593.71)
         (419.71, 906.54)
         (559.61, 1253.17)
         (699.51, 1468.55)
         (839.42, 1831.36)
    }; 
\node [below] at (axis cs:  139.90, 254.23) {$254.23$};
\node [right] at (axis cs:  279.80, 593.71) {$593.71$};
\node [right] at (axis cs: 419.71, 906.54) {$906.54$};
\node [left] at (axis cs: 559.61, 1253.17) {$1253.17$};
\node [left] at (axis cs: 699.51, 1468.55) {$1468.55$};
\node [left] at (axis cs: 839.42, 1831.36) {$1831.36$};
\addlegendentry{$Hadoop$  cluster}
\addplot[color=red,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 859.70)
         (279.80, 907.73)
         (419.71, 959.61)
         (559.61, 1154.24)
         (699.51, 1208.77)
         (839.42, 1338.82)
    }; 
\node [below] at (axis cs:  139.90, 859.70) {$859.70$};
\node [above] at (axis cs:  279.80, 907.73) {$907.73$};
\node [above] at (axis cs: 419.71, 959.61) {$959.61$};
\node [right] at (axis cs: 559.61, 1042.10) {$1042.10$};
\node [right] at (axis cs: 699.51, 1112.97) {$1112.97$};
\node [above] at (axis cs: 839.42,1196.68) {$1196.68$};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 5 worker nodes by each node has CPU 4 cores and memory 8 GB}
\end{figure}
From Fig 5, we can analyze that the intersection point between Hadoop cluster line and Spark cluster line at the size of dataset is 462.38 MB and the execution time is 990.78 seconds. As the result, we can execute parallel collaborative filtering on Hadoop cluster faster than Spark cluster when the size of input data is less than 462.38 MB by 5 worker nodes by each node has CPU 4 cores and memory 8 gb.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	xtick={139.90, 279.80, 419.71, 559.61, 699.51, 839.42},
	grid=both, 
	legend pos=north west]
\addplot[color=blue,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 224.59)
         (279.80, 504.72)
         (419.71, 828.13)
         (559.61, 1083.10)
         (699.51, 1441.27)
         (839.42, 1724.50)
    }; 
\node [below] at (axis cs:  139.90, 224.59) {$224.59$};
\node [right] at (axis cs:  279.80, 504.72) {$504.72$};
\node [right] at (axis cs: 419.71, 828.13) {$828.13$};
\node [left] at (axis cs: 559.61, 1083.10) {$1083.10$};
\node [left] at (axis cs: 699.51, 1441.27) {$1441.27$};
\node [left] at (axis cs: 839.42, 1724.50) {$1724.50$};
\addlegendentry{$Hadoop$  cluster}
\addplot[color=red,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 756.19)
         (279.80, 841.94)
         (419.71, 907.18)
         (559.61, 968.92)
         (699.51, 1033.82)
         (839.42, 1081.29)
    }; 
\node [below] at (axis cs:  139.90, 756.19) {$756.19$};
\node [above] at (axis cs:  279.80, 841.94) {$841.94$};
\node [above] at (axis cs: 419.71, 907.18) {$907.18$};
\node [right] at (axis cs: 559.61, 968.92) {$968.92$};
\node [right] at (axis cs: 699.51, 1033.82) {$1033.82$};
\node [above] at (axis cs: 839.42,1081.29) {$1081.29$};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 4 worker nodes by each node has CPU 5 cores and memory 10 GB}
\end{figure}
From Fig 6, we can analyze that the intersection point between Hadoop cluster line and Spark cluster line at the size of dataset is 468.33 MB and the execution time is 921.70 seconds. As the result, we can execute parallel collaborative filtering on Hadoop cluster faster than Spark cluster when the size of input data is less than 468.33 MB by 4 worker nodes by each node has CPU 5 cores and memory 10 gb.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	xtick={139.90, 279.80, 419.71, 559.61, 699.51, 839.42},
	grid=both, 
	legend pos=north west]
\addplot[color=blue,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 202.19)
         (279.80, 473.55)
         (419.71, 727.21)
         (559.61, 927.78)
         (699.51, 1257.95)
         (839.42, 1571.32)
    }; 
\node [below] at (axis cs:  139.90, 202.19) {$202.19$};
\node [right] at (axis cs:  279.80, 473.55) {$473.55$};
\node [right] at (axis cs: 419.71, 727.21) {$727.21$};
\node [left] at (axis cs: 559.61, 927.78) {$927.78$};
\node [left] at (axis cs: 699.51, 1257.95) {$1257.95$};
\node [left] at (axis cs: 839.42, 1571.32) {$1571.32$};
\addlegendentry{$Hadoop$  cluster}
\addplot[color=red,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 699.15)
         (279.80, 769.21)
         (419.71, 805.36)
         (559.61, 842.45)
         (699.51, 907.80)
         (839.42, 954.81)
    }; 
\node [below] at (axis cs:  139.90, 699.15) {$699.15$};
\node [above] at (axis cs:  279.80, 769.21) {$769.21$};
\node [above] at (axis cs: 419.71, 805.36) {$805.36$};
\node [right] at (axis cs: 559.61, 842.45) {$842.45$};
\node [right] at (axis cs: 699.51, 907.80) {$907.80$};
\node [above] at (axis cs: 839.42, 954.81) {$954.81$};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 4 worker nodes by each node has CPU 5 cores and memory 10 GB}
\end{figure}
From Fig 7, we can analyze that the intersection point between Hadoop cluster line and Spark cluster line at the size of dataset is 470.37 MB and the execution time is 822.98 seconds. As the result, we can execute parallel collaborative filtering on Hadoop cluster faster than Spark cluster when the size of input data is less than 470.37  MB by 2 worker nodes by each node has CPU 10 cores and memory 20 gb.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	xtick={139.90, 279.80, 419.71, 559.61, 699.51, 839.42},
	grid=both, 
	legend pos=north west]
\addplot[color=blue,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 173.45)
         (279.80, 446.65)
         (419.71, 663.96)
         (559.61, 898.30)
         (699.51, 1154.64)
         (839.42, 1498.75)
    }; 
\node [below] at (axis cs:  139.90, 173.45) {$173.45$};
\node [right] at (axis cs:  279.80, 446.65) {$446.65$};
\node [right] at (axis cs: 419.71, 663.96) {$663.96$};
\node [left] at (axis cs: 559.61, 898.30) {$898.30$};
\node [left] at (axis cs: 699.51, 1154.64) {$1154.64$};
\node [left] at (axis cs: 839.42, 1498.75) {$1498.75$};
\addlegendentry{$Hadoop$  cluster}
\addplot[color=red,solid,thick,mark=*, mark options={fill=white}] 
    coordinates {
         (139.90, 674.95)
         (279.80, 721.38)
         (419.71, 759.26)
         (559.61, 796.20)
         (699.51, 829.71)
         (839.42, 893.00)
    }; 
\node [below] at (axis cs:  139.90, 674.95) {$674.95$};
\node [above] at (axis cs:  279.80, 721.38) {$721.38$};
\node [above] at (axis cs: 419.71, 759.26) {$759.26$};
\node [right] at (axis cs: 559.61, 796.20) {$796.20$};
\node [right] at (axis cs: 699.51, 829.71) {$829.71$};
\node [above] at (axis cs: 839.42, 893.00) {$893.00$};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 4 worker nodes by each node has CPU 5 cores and memory 10 GB}
\end{figure}
From Fig 8, we can analyze that the intersection point between Hadoop cluster line and Spark cluster line at the size of dataset is 472.19 MB and the execution time is 773.9 seconds. As the result, we can execute parallel collaborative filtering on Hadoop cluster faster than Spark cluster when the size of input data is less than 472.19  MB by 1 worker node which has CPU 20 cores and memory 40 gb.\\

\begin{table}[h]
\centering
\caption{The gradient of cluster line by each configuration that observe parallel I/O }
\label{my-label}
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{The configuration in cluster} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Hadoop\\ cluster\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Spark\\ cluster\end{tabular}} \\
 &  &  \\ \hline
\begin{tabular}[c]{@{}c@{}}5 worker nodes by each node \\ has CPU 4 cores and memory 8 gb\end{tabular} & 2.25 & 0.48 \\ \hline
\begin{tabular}[c]{@{}c@{}}4 worker nodes by each node\\ has CPU 5 cores and memory 10 gb\end{tabular} & 2.14 & 0.46 \\ \hline
\begin{tabular}[c]{@{}c@{}}2 worker nodes by each node \\ has CPU 10 cores and memory 20 gb\end{tabular} & 1.96 & 0.37 \\ \hline
\begin{tabular}[c]{@{}c@{}}1 worker node which \\ has CPU 20 cores and memory 40 gb\end{tabular} & 1.89 & 0.31 \\ \hline
\end{tabular}
\end{table} 

\indent After execute the multiple linear regression algorithm as the equation (2), we discover that the coefficient of size of dataset is 0.3997 on Spark cluster and 2.032 on Hadoop cluster, the coefficient of the number of worker nodes is 56.9851 on Spark cluster and 59.832 on Hadoop cluster, the interception is 521.7023 on Spark cluster and -253.391 on Hadoop cluster.

\begin{table}[h]
\centering
\caption{The value of coefficient in Multiple Linear Regression}
\label{my-label}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
The kind of cluster & \begin{math}\alpha_{1}\end{math} & \begin{math}\alpha_{2}\end{math} & \begin{math}\beta_{1}\end{math} & \begin{math}\beta_{2}\end{math} & \begin{math}\beta_{3}\end{math} & \begin{math}\beta{4}\end{math} \\ \hline
Hadoop cluster & 537.26 & -253.39 & -70.89 & 1.94 & 59.83 & 2.03 \\ \hline
Spark cluster & 927.15 & 521.70 & -38.54 & 0.51 & 56.99 & 0.40 \\ \hline
\end{tabular}
\end{table}
The Multiple Linear Regression (MLR) equation

\begin{itemize}
\item{Spark cluster : }
\begin{equation} \tag{3.1}
	t = 927.15 - 38.54c + 0.51d
\end{equation}
\begin{equation} \tag{4.1}
	t = 521.70 + 56.99n + 0.40d 
\end{equation}
\item{Hadoop cluster : }
\begin{equation} \tag{3.2}
	t = 537.26 - 70.89c + 1.94d
\end{equation}
\begin{equation} \tag{4.2}
	t = -253.39 + 59.83n + 2.03d 
\end{equation}
\end{itemize}

\section{Discussion}
The purpose of this research was to find the suitable configuration of Spark cluster and Hadoop cluster to execute parallel collaborative filtering technique for create recommendation system and discover the impact from parallel computing and parallel I/O on the performance.\\
\indent To observe the parallel computing impact on performance by changing the number of CPU cores per each worker node with fix the size of memory.\\
\indent To observe the parallel I/O impact on performance by changing the number of worker nodes with the same total number of CPU cores and size of memory.

\section{Conclusion}









\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{picture}}]{John Doe}
\blindtext
\end{IEEEbiography}


\end{document}


