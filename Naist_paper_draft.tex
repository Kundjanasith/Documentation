
\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}
\usepackage{multirow}
\usepackage{pgfplotstable}
\usepackage{pgfplots}
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
\title{Performance Analysis using Large-scale Parallel Collaborative Filtering on Spark and Hadoop}

\author{\IEEEauthorblockN{Kundjanasith Thonglek}
\IEEEauthorblockA{High Performance Computing and Network Center\\
Deparment of Computer Engineering, Kasetsart University\\
Bangkok, Thailand\\
kundjanasith.t@ku.th}
\and
\IEEEauthorblockN{Kohei Ichikawa}
\IEEEauthorblockA{Software Design and Analysis Laboratory\\
Graduate School of Information Science, NAIST\\
Nara, Japan\\
ichikawa@is.naist.jp}}
%\and
%\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%\IEEEauthorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}
\maketitle


\begin{abstract}
Spark is a cluster computing technology for large-scale data processing that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Following the SparkContext can connect to several types of cluster manager which allocate resource across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for the application. Next, it sends application code to the executors. SparkContext represents the connection to a Spark cluster and can be used to create resilient distributed datasets (RDDs). it also accumulators and broadcast variables on the cluster. This research approaches the problem by comparing Spark cluster and Hadoop cluster performance are used the parallel collaborative filtering program?s execution time as criteria to find when the application should execute on Spark or Hadoop. This research is primarily intended for cluster computing benchmark.
\end{abstract}

\begin{IEEEkeywords}
Large scale data processing; Resilient Distributed Datasets; Collaborative Filtering; Spark;
\end{IEEEkeywords}


\section{Introduction}
Large scale data processing is the process of applying data analysis techniques to a large amount of data. Typically, large scale data analysis is performed through two popular techniques: parallel database management system (DBMS) or MapReduce powered systems. The parallel database management system requires that the data be in a DBMS supported schema, whereas the MapReduce option supports data in any form. Moreover, the data extracted or analyzed in large-scale data analysis can be displayed in various different forms, such as tables, graphs, figures and statistical analysis, depending on the analysis system. One of the popular engines for large scale data processing based on the MapReduce model is Spark.\\
\indent In Spark cluster, there are two types of nodes: driver node and worker node. The driver node is one of the critical components of the cluster which maintains state information of all worker nodes attached to the cluster and also responsible for maintaining the SparkContext. The worker node runs the Spark executors and other critical services required for the proper functioning of the cluster when distribute workload with Spark, all the distributed processing happens on these workers.\\
\indent In Hadoop cluster, there are three types of node: client node ,master node and slave node. The client node is neither master or slave, rather play a role of loading the data into the cluster, submit MapReduce jobs describing how to the data should be processed and then retrieve the data to see the response after job completion. The master node consists of three components name node that oversees the health of data node and coordinates access to the data stored in a data node, a secondary node that is to contact name node in a periodic manner after certain time interval and job tracker that coordinates the parallel processing of data using MapReduce. The slave node consists of two components data node and task tracker. The data node that stores the data and process the computation. The task tracker that communicates to their masters.

%\subsection{Subsection Heading Here}
%\blindtext

\section{Literature review}

%\begin{itemize}
%\item Cold start is a large amount of existing data that necessary to make accurate recommendations for a given user.
%\item Scalability is a large amount of computational power that necessary to compute timely recommendations
%\item Sparsity is the number of items typically far exceeds the number of users making our relations extremely sparse as most active users will have only rated a small subset of the total items.
%\end{itemize}
\indent Parallel Collaborative Filtering technique on Spark cluster and Hadoop cluster
\subsection{Large-scale Parallel Collaborative Filtering for Netflix}
\indent This paper describe about why they use parallel collaborative filtering to creating recommendation system for Netflix dataset by using Matlab library and Linux cluster. \\
\indent This paper doesn?t describe about when we execute parallel collaborative filtering using Spark MLlib on Spark cluster and Mahout on Hadoop cluster

\subsection{Comparing Apache Spark and MapReduce with Performance Analysis using K-means}
\indent This paper describe about the performance comparison between Spark  and MapReduce using K-means clustering algorithm on 1 node and 2 nodes. \\
\indent This paper doesn?t describe about when we execute alternating least square algorithm on Spark and Hadoop cluster so we will conduct the experiment to execute the algorithm on both cluster

\section{Design\&Implementation}

\section{Experimental result}
\indent The experimental result is divided into 2 sections :\\
\indent First section is the experimental result of executing parallel collaborative filtering using alternating least square algorithm with Spark ML library on Spark cluster and Mahout library on Hadoop cluster by changing the number of CPU cores per each worker node in the cluster which has 1 master node and 5 worker nodes for observe the impact of parallel computing\\


\begin{table}[]
\centering
\caption{Result of testing on 5 workers node \protect\\ by each node has CPU 2 cores and memory 8 GB}
\label{my-label}
\begin{tabular}{|c|r|r|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Size of dataset\\ ( MB )\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Execution time on\\ Hadoop ( seconds )\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Execution time on\\ Spark (seconds )\end{tabular}}} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline
139.90 & 463.88 & 1018.69 \\ \hline
279.80 & 920.81 & 1061.27 \\ \hline
419.71 & 1437.65 & 1099.85 \\ \hline
559.61 & 1900.33 & 1154.24 \\ \hline
699.51 & 2337.72 & 1208.77 \\ \hline
839.42 & 2753.91 & 1338.82 \\ \hline
\end{tabular}
\end{table}

\centering
\begin{figure}
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	legend pos=north west]
\addplot table [y=$Hadoop$, x=Size]{latex_data/HS_C1.dat};
\addlegendentry{$Hadoop$  cluster}
\addplot table [y=$Spark$, x=Size]{latex_data/HS_C1.dat};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 5 workers node by each node has CPU 2 cores and memory 8 GB}
\end{figure}


\begin{table}[]
\centering
\caption{Result of testing on 5 workers node \protect\\ by each node has CPU 4 cores and memory 8 GB}
\label{my-label}
\begin{tabular}{|c|r|r|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Size of dataset\\ ( MB )\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Execution time on\\ Hadoop ( seconds )\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Execution time on\\ Spark (seconds )\end{tabular}}} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline
139.90 & 254.23 & 813.04 \\ \hline
279.80 & 593.71 & 907.73 \\ \hline
419.71 & 906.54 & 959.61 \\ \hline
559.61 & 1253.17 & 1042.10 \\ \hline
699.51 & 1468.55 & 1112.97 \\ \hline
839.42 & 1831.36 & 1196.68 \\ \hline
\end{tabular}
\end{table}

\centering
\begin{figure}
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	legend pos=north west]
\addplot table [y=$Hadoop$, x=Size]{latex_data/HS_C2.dat};
\addlegendentry{$Hadoop$  cluster}
\addplot table [y=$Spark$, x=Size]{latex_data/HS_C2.dat};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 5 workers node by each node has CPU 4 cores and memory 8 GB}
\end{figure}

\begin{table}[]
\centering
\caption{Result of testing on 5 workers node \protect\\ by each node has CPU 8 cores and memory 8 GB}
\label{my-label}
\begin{tabular}{|c|r|r|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Size of dataset\\ ( MB )\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Execution time on\\ Hadoop ( seconds )\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Execution time on\\ Spark (seconds )\end{tabular}}} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline
139.90 & 174.88 & 613.34 \\ \hline
279.80 & 351.64 & 663.38 \\ \hline
419.71 & 550.53 & 741.04 \\ \hline
559.61 & 727.93 & 833.40 \\ \hline
699.51 & 921.13 & 903.04 \\ \hline
839.42 & 1092.84 & 993.54 \\ \hline
\end{tabular}
\end{table}

\centering
\begin{figure}
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	legend pos=north west]
\addplot table [y=$Hadoop$, x=Size]{latex_data/HS_C3.dat};
\addlegendentry{$Hadoop$  cluster}
\addplot table [y=$Spark$, x=Size]{latex_data/HS_C3.dat};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 5 workers node by each node has CPU 8 cores and memory 8 GB}
\end{figure}


\begin{table}[]
\centering
\caption{Result of testing on 5 workers node \protect\\ by each node has CPU 16 cores and memory 8 GB}
\label{my-label}
\begin{tabular}{|c|r|r|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Size of dataset\\ ( MB )\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Execution time on\\ Hadoop ( seconds )\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Execution time on\\ Spark (seconds )\end{tabular}}} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline
139.90 & 160.47 & 409.36 \\ \hline
279.80 & 323.76 & 475.68 \\ \hline
419.71 & 438.17 & 543.79 \\ \hline
559.61 & 549.12 & 612.68 \\ \hline
699.51 & 678.46 & 708.02 \\ \hline
839.42 & 818.09 & 798.31 \\ \hline
\end{tabular}
\end{table}

\centering
\begin{figure}
\begin{tikzpicture}
\begin{axis}[
	xlabel=Size of dataset ( MB ),
	ylabel=TIme ( seconds ), 
	legend pos=north west]
\addplot table [y=$Hadoop$, x=Size]{latex_data/HS_C4.dat};
\addlegendentry{$Hadoop$  cluster}
\addplot table [y=$Spark$, x=Size]{latex_data/HS_C4.dat};
\addlegendentry{$Spark$ cluster}
\end{axis}
\end{tikzpicture}
\caption{Graph of testing on 5 workers node by each node has CPU 16 cores and memory 8 GB}
\end{figure}

\indent Second section is the experimental result of executing parallel collaborative filtering using alternating least square algorithm with Spark ML library on Spark cluster and Mahout library on Hadoop cluster by changing the number of worker nodes in the cluster has the total CPU 20 cores and memory 40 GB for observe the impact of parallel I/O\\

\section{Discussion}

\section{Conclusion}




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Proof of the First Zonklar Equation}
\blindtext

% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{picture}}]{John Doe}
\blindtext
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}




% that's all folks
\end{document}


